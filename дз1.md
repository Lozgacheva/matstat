Лозгачёва Анна СКБ222
В работе за $X$ одинаково обозначается случайная величина, имеющая соответствующее пункту распределение.
# Логарифмическое распределение
$P_{X} (x) = -\ln(1-\theta)^{-1} \cdot \theta^{x} \cdot x^{-1} \quad , \quad x \in \mathbb{N}, \ 0<\theta<1$
### 1. Описание основных характеристик распределения
###### Функция распределения
$F_{X}(x) = \sum^{x}_{k=1} \frac{-1}{ln(1-\theta)} \frac{\theta^{k}}{k} = \frac{-1}{ln(1-\theta)} \sum^{x}_{k=1} \frac{\theta^{k}}{k}$

###### Математическое ожидание
$E X = \sum^{\infty}_{x=1} x \frac{-1}{ln(1-\theta)}  \frac{\theta^{x}}{x} = \sum^{\infty}_{x=1}  \frac{-1}{ln(1-\theta)} \theta^{x} = \frac{-\theta}{ln(1-\theta)} \sum^{\infty}_{x=1}\theta^{x-1} = \frac{-\theta}{(1-\theta)ln(1-\theta)}$

###### Дисперсия
$EX^{2} = \sum^{\infty}_{x=1} x^{2} \frac{-1}{ln(1-\theta)}  \frac{\theta^{x}}{x} =  \frac{-1}{ln(1-\theta)} \sum^{\infty}_{x=1}  x \theta^{x} = \left\{ c= \frac{-1}{ln(1-\theta)} \right\} =$
$=c \sum_{x=1}^{\infty} (x+1) \theta^{x} - \theta^{x} = c \left(\sum^{\infty}_{x=1} \theta ^ {x+1}\right)^{`} -c \sum_{x=1}^{\infty} \theta^{x} =c\left(\theta^{2} \sum_{x=1}^{\infty}\theta^{x-1}\right)^{`} - c \frac{\theta}{1-\theta}=$
$=c\left(\frac{\theta^{2}}{1-\theta}\right)^{`} - c \frac{\theta}{1-\theta} = c \frac{2 \theta(1-\theta)+\theta^{2}} {(1-\theta)^{2}}- c\frac{ \theta}{ 1-\theta} = \frac{2 \theta c - 2 \theta^{2} c + \theta^{2} c - c \theta + c \theta^{2}}{(1-\theta)^{2}}=$
$=\frac{\theta c}{(1-\theta)^{2}}= \frac{-\theta}{(1-\theta)^{2}ln(1-\theta)}$

$DX=EX^{2} -(EX)^{2} = \frac{-\theta}{(1-\theta)^{2}ln(1-\theta)} - \left(\frac{-\theta}{(1-\theta)ln(1-\theta)}\right)^{2} =$
$=\frac{-\theta ln(1-\theta)}{(1-\theta)^{2}ln^{2}(1-\theta)} - \frac{\theta^{2}}{(1-\theta)^{2}ln^{2}(1-\theta)}=\frac{-\theta \ln(1-\theta) - \theta^{2}}{(1-\theta)^{2}ln^{2}(1-\theta)}= -\theta \frac{ \ln(1-\theta) + \theta}{(1-\theta)^{2}ln^{2}(1-\theta)}$

###### Квантиль уровня $\gamma$
$F(x_{\gamma}) \geq \gamma$
$\gamma \leq \frac{-1}{ln(1-\theta)} \sum^{x_\gamma}_{k=1} \frac{\theta^{k}}{k}$

### Поиск примеров событий, которые могут быть описаны выбранными случайными величинами

###### Известные соотношения между распределениями
Пусть $Y = \sum^{N}_{i=1} X_{i}$, где $X_{i} \sim log(p)$. Пусть также $N \sim П(\lambda)$. Производящая функция $X_{i}$ равна $\frac{\ln (1-ps)}{\ln(1-p)}$. 
Тогда производящая функция $Y$ равна 
$$
\exp\left({\lambda\left(\frac{\ln (1-ps)}{\ln(1-p)} - 1\right)}\right)=\left(\frac{1-p}{1-ps}\right) ^ {\frac{-\lambda}{ln(1-p)}}
$$
что является производящей функция для $NB\left(\frac{-\lambda}{\ln(1-p)}, 1-p\right)$

### Описание способа моделирования выбранных случайных величин

Методом обратного преобразования. 
1) Сгенерируем $U \sim R(0, 1)$
2) Найдем индекс k такой, что $\sum_{j=1}^{k-1} p_{j} \leq U < \sum_{j=1} ^{k} p_{j}$ и возвращаем $X=x_{k}$



# Нормальное распределение
$f_{X}(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^{2}\right)$
### Описание основных характеристик распределения
###### Функция распределения
$F_{X}(x) = \int^{x}_{-\infty} \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{1}{2} \left(\frac{t-\mu}{\sigma}\right)^{2}\right)dt =$
$=\frac{1}{{\sigma \sqrt{2 \pi}}} \int^{x}_{-\infty}  \exp \left(-\frac{1}{2} \left(\frac{t-\mu}{\sigma}\right)^{2}\right)dt = \frac{1}{{\sqrt{2 \pi}}} \int^{x}_{-\infty} \exp \left(\frac{-u^{2}}{2}\right)du$

###### Математическое ожидание
$EX = \frac{1}{\sigma \sqrt{2 \pi}} \int_{- \infty}^{\infty}x e^{-\frac{1}{2} \left(\frac{x-a}{\sigma}\right)^{2}} dx = \left| \begin{aligned} & t = \frac{x-\mu}{\sigma} \\ & dx= \sigma dt \\ \end{aligned} \right| = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} (t \sigma + \mu) e^{- \frac{1}{2} t^{2}}dt=$
$=\frac{\mu}{\sqrt{2 \pi}} \int^{\infty}_{-\infty} e^{\frac{-t^{2}}{2}} = \mu$

###### Дисперсия
$EX^{2} = \frac{1}{{ \sqrt{2 \pi}}} \int_{-\infty}^{\infty} (\sigma^{2} t^{2} + 2t \sigma \mu + \mu^{2})e^{\frac{-t^{2}}{2}}dt =$
$=\frac{1}{\sqrt{2 \pi}} \left(\sigma^{2} \underbrace{ \int_{-\infty}^{\infty}t^{2} e^{\frac{-t^{2}}{2}}dt }_{(1)} + \mu^{2} \int_{-\infty}^{\infty} e^{\frac{-t^{2}}{2}} dt \right)=\sigma^{2} + \mu^{2}$

$(1) \quad \int_{-\infty}^{\infty}t^{2} e^{\frac{-t^{2}}{2}}dt = \left| \begin{aligned} & u = t \quad du=dt \\& dv = te^{\frac{-t^{2}}{2}} dt  \\ & v = -e^{\frac{-t^{2}}{2}}  \end{aligned} \right| = \underbrace{\left. -te^{\frac{-t^{2}}{2}} \right|_{-\infty}^{\infty}}_{=0} + \int_{-\infty}^{\infty} e^{\frac{-t^{2}}{2}} dt = \sqrt{2 \pi}$
$DX = EX^{2} - (EX)^{2} = \sigma^{2} + \mu^{2} - \mu^{2} = \sigma^{2}$
###### Квантиль уровня $\gamma$
$F_{X} (x_{\gamma}) = \gamma =\frac{1}{{\sqrt{2 \pi}}} \int^{x_\gamma}_{-\infty} \exp \left(\frac{-u^{2}}{2}\right)du$

### Поиск примеров событий, которые могут быть описаны выбранными случайными величинами
###### Привести пример интерпретации распределения – описания события, исходы в котором подчиняются выбранному распределению

Допустим, что какая-то компания хочет сделать рекламную рассылку по n своим клиентам. Допустим также, что вероятность того, что клиент посмотрит письмо равна $p$, а вероятность того, что проигнорирует $q=1-p$. 
Число просмотров рассылки будет подчиняться биномиальному распределению:
${P}(X=x)=\binom{n}{x} p^x q^{n-x}$
Тогда, если рассматривать эту же ситуацию при ${} n, x \rightarrow \infty {}$, то можем воспользоваться теоремой Муавра-Лапласа:
$P(X=k)=\binom{n}{k} p^k(1-p)^{n-k} = \frac{1}{\sqrt{2 \pi n p(1-p)}} \cdot \exp \left(-\frac{(k-n p)^2}{2 n p(1-p)}\right)$
$f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)$
Получили ничто иное, как плотность нормального распределения с параметрами $\mu=np$ и $\sigma^{2} = np(1-p)$, которому и будет подчиняться ситуация, описанная в примере. Таким образом, нормальное распределение описывает количество просмотров рекламы из приведенного примера.

###### Известные соотношения между распределениями
В предыдущем пункте также показана связь с биномиальным распределением.

### Описание способа моделирования выбранных случайных величин
[Преобразование Бокса-Мюллера](https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B5%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%91%D0%BE%D0%BA%D1%81%D0%B0_%E2%80%94_%D0%9C%D1%8E%D0%BB%D0%BB%D0%B5%D1%80%D0%B0)
Пусть $r$ и $\varphi$ - независимые случайные величины, равномерно распределенные на интервале $(0,1]$. Вычислим $z_0$ и $z_{1}$: 
$\begin{aligned} & z_0=\cos (2 \pi \varphi) \sqrt{-2 \ln r} \\ & z_1=\sin (2 \pi \varphi) \sqrt{-2 \ln r}\end{aligned}$
Тогда $z_{0}$ и $z_{1}$ будут независимы и распределены нормально с математическим ожиданием 0 и дисперсией 1.