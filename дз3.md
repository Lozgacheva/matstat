**Теорема(Рао - Блекуэлла - Колмогорова)** Оптимальная оценка, если она и существует, явялется функцией от достаточной статистики \

**Опр** Достаточную статистику называют полной, если для функции $\phi(T)$ из того, что

$$E_\theta \phi(T) = 0$$ для любого $\theta$ следует $\phi(t) \equiv 0$ на всем множестве значений статистики T \

**Теорема** Если существует полная достаточная статистика, то всякая функция от нее является оптимальной оценкой своего математического ожидания

**Критерий Рао-Крамера**

$$T(X) - \tau(\theta) = \alpha(\theta)V(X,\theta)$$, где $\alpha(\theta)$ -некоторая функция от $\theta$


# Нормальное распределение $\mathcal{N}(\mu, \hat{\theta}^{2})$
$f(x) = \frac{1}{{{\theta} \sqrt{2 \pi}}} \exp \left(- \frac{(x-\mu)^{2}}{2{\theta}^{2}}\right), x, \mu \in \mathbb{R}, {\theta}>0$ 
###### Оценка максимального правдоподобия
$L=L(X, {\theta}) = \prod^{n}_{i=1} f(x_{i}| {\theta}) = \prod^{n}_{i=1} \frac{1}{{{\theta} \sqrt{2 \pi}}} \exp \left(- \frac{(x_{i}-\mu)^{2}}{2{\theta}^{2}}\right) =\frac{1}{({{\theta} \sqrt{2 \pi}})^{n}} \exp \left(- \sum^{n}_{i=1} \frac{(x_{i}-\mu)^{2}}{2{\theta}^{2}}\right)$
$\ln L = \ln \frac{1}{({\theta} \sqrt{2 \pi})^{n}} - \sum^{n}_{i=1}\frac{(x_{i} -\mu)^{2}}{2{\theta}^{2}} = -n \ln {\theta} -n \ln \sqrt{2 \pi} - \frac{1}{2 {\theta}^{2}} \sum_{i=1}^{n} (x_{i} - \mu)^{2}$ 
$\frac{\partial L}{\partial {\theta}} = \frac{-n}{{\theta}} + \frac{1}{{\theta}^{3}} \sum_{i=1}^{n} (x_{i} - \mu)^{2} = 0$
$\frac{-n}{\hat{\theta}} + \frac{1}{\hat{\theta}^{3}} \sum_{i=1}^{n} (x_{i} - \mu)^{2} = 0$
$\frac{1}{\hat{\theta}^{2}} \sum_{i=1}^{n} (x_{i} - \mu)^{2} = n$
$\hat{\theta}^{2} = \frac{1}{n} \sum_{i=1}^{n} (x_{i} - \mu)^{2}$

###### Оценка методом моментов
$$
\begin{flalign}
& a_{1} = \mu \\
& \hat{a}_{1} = \frac{1}{n} \sum^{n}_{i=1} x_{i} = \mu\\
& a_{2} = {\theta}^{2} + \mu^{2} \\
& \hat{a}_{2} = \frac{1}{n} \sum^{n}_{i=1} x_{i}^{2} = {\theta}^{2} + \mu^{2}\\
& \hat{\theta}^{2} = \hat{a}_{2} - \hat{a}_{1}^{2} = S^{2} \\
&&
\end{flalign}
$$
###### Оптимальная оценка
Нормальная модель регулярна $\Rightarrow$ можем найти эффективную оценка, которая также является оптимальной.
$$
\begin{flalign}
& L(X,{\theta}) = L= \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi}{\theta}} \exp\left(- \frac{(x-\mu)^{2}}{2{\theta}^{2}}\right) \\
& \ln L = \sum_{i=1}^{n}\left[\ln \frac{1}{\sqrt{2 \pi}{\theta}} - \frac{(x-\mu)^{2}}{2{\theta}^{2}}\right] = \sum_{i=1}^{n} \left[-\ln \sqrt{2 \pi}{\theta} - \frac{(x-\mu)^{2}}{2{\theta}^{2}}\right] \\
& V({\theta}) = V = \frac{\partial \ln L}{\partial {\theta}} = \sum_{i=1}^{n}\left[\frac{-1}{{\theta}} + \frac{(x-\mu)^{2}}{{\theta}^{3}}\right] = \frac{-n}{{\theta}} + \frac{1}{{\theta}^{3}} \sum_{i=1}^{n} (x-\mu)^{2} \\
& -{\theta}^{2}n + \sum_{i=1}^{n} (x-\mu)^{2} = {\theta}^{3} V \\
& \frac{1}{n} \sum_{i=1}^{n} (x-\mu)^{2} - {\theta}^{2} = \frac{{\theta}^{3}V}{n}\\
&&
\end{flalign}
$$
Получили, что $\hat\theta = \frac{1}{n} \sum_{i=1}^{n} (x-\mu)^{2}$ - линейная функция вклада выборки, а следовательно является эффективной, а следовательно и оптимальной для $\tau ({\theta}^{2})$.


Построим оценку для $\theta$

Известна связь с $\chi^2$

$$\frac{\sum_{i=1}^n (X_i - \mu)^2}{\theta^2} \sim \chi_n^2 \implies \sqrt{\frac{\sum_{i=1}^n (X_i - \mu)^2}{\theta^2}} \sim \chi_n$$
$$
\sqrt{n}Y \sim \chi_{n}
$$
$E_\xi = \sqrt{2}\frac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac{n}{2})}, \xi \sim \chi_n$

$$\sqrt{T(x)} = \sqrt{\frac{\sum_{i=1}^n (X_i - \mu)^2}{n^2}} = \theta \frac{Y}{\sqrt{n}}$$
$Y = \sqrt{\frac{\sum_{i=1}^n (X_i - \mu)^2}{\theta^2}} \sim \chi_n$

$$E_{\sqrt{T(x)}} = E\theta \frac{Y}{\sqrt{n}} = \theta \frac{\sqrt{2}\frac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac{n}{2})}}{\sqrt{n}}$$

Тогда наша оптимальная, т.к. построена от полной достаточной, оценка есть

$$\theta = \sqrt{T(x)} \frac{\sqrt{n}\frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n+1}{2})}}{\sqrt{2}}= \sqrt{\frac{\sum_{i=1}^n (X_i - \mu)^2}{2}} \cdot \frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n+1}{2}\right)}$$
# Логарифмическое распределение
$P(x) = \frac{-1}{\ln(1-{\theta})} \frac{{\theta}^{x}}{x}$
###### Оценка максимального правдоподобия
$$
\begin{flalign}
& L = \prod^{n}_{i=1} \frac{-{\theta}^{x}}{\ln(1-{\theta})x_{i}} \\
& \ln L = \sum_{i=1}^{n} \ln \frac{-{\theta}^{x_{i}}}{x_{i}\ln(1-{\theta})} =\empheqlbrack \sum_{i=1}^{n} \ln -{\theta}^{x_{i}} - \ln x_{i} - \ln \ln(1-{\theta}) \empheqrbrack  \\
& \frac{\partial \ln L}{\partial {\theta}} = \sum_{i=1}^{n}\left[ \frac{-{\theta}^{x_{i}-1} x_{i}}{-{\theta}^{x_{i}}} + \frac{1}{(1-{\theta})\ln(1-{\theta})}\right] \\
& \sum_{i=1}^{n} \frac{-\hat{\theta}^{x_{i}-1} x_{i}}{-\hat{\theta}^{x_{i}}} + \frac{n}{(1-\hat{\theta})\ln(1-\hat{\theta})} = 0 \\
& \sum_{i=1}^{n} \frac{-\hat{\theta}^{x_{i}-1}x_{i}}{-\hat{\theta}^{x_{i}}} = \frac{-n}{(1-\hat{\theta})\ln(1-\hat{\theta})} \\
& \stackrel{||}{\sum_{i=1}^{n} \frac{x_{i}}{\hat{\theta}}}  = \frac{1}{\hat{\theta}} \sum_{i=1 }^{n}x_{i} \\
& \frac{1}{\hat{\theta}} \sum_{i=1 }^{n}x_{i} =  \frac{-n}{(1-\hat{\theta})\ln(1-\hat{\theta})} \\
& \overline{x} = \frac{1}{n} \sum_{i=1 }^{n} x_{i} = \frac{-\hat{\theta}}{(1-\hat{\theta})\ln(1-\hat{\theta})}
&&
\end{flalign}
$$
$\overline{x} = \frac{-\hat{\theta}}{(1-\hat{\theta})\ln(1-\hat{\theta})}$

Сделаем замену $y=\frac{\hat{\theta}}{1-\hat{\theta}} \Rightarrow \hat{\theta} = \frac{y}{1+y}, \ 1-\hat{\theta} = \frac{1}{1+y}$.
$$
\begin{flalign}
& \overline{x} = \frac{-y}{(1+y)\left(\frac{1}{1+y}\right)\ln(\frac{1}{1+y})} = \frac{-y}{-\ln(1+y)} = \frac{y}{\ln(1+y)} \\
& \overline{x} = \frac{1}{\ln(1+y)} \\
& y = \overline{x} \ln(1+y)\\
& \frac{y}{x} = \ln(1+y) \\
& \exp\left({\frac{y}{\overline{x}}}\right)= \exp(\ln(1+y)) \\
& \exp\left(\frac{y}{\overline{x}}\right)= 1+y  \\
& \\
& \frac{1+y}{\exp(\frac{y}{\overline{x}})} = 1 \\
& (1+y)\exp\left(\frac{y}{-\overline{x}}\right) = 1 \\
& \frac{1+y}{-\overline{x}} \exp\left(\frac{y+1}{-\overline{x}}\right) = \frac{\exp\left(\frac{1}{-\overline{x}}\right)}{-\overline{x}}\\
& \frac{1+y}{-\overline{x}} = W\left(\frac{\exp\left(\frac{-1}{x}\right)}{-\overline{x}}\right),
&&
\end{flalign}
$$
где $W$ - функция Ламберта - обратная функция к $f(x)=xe^{x}$ для комплексных $x$.
$$
\begin{flalign}
& y=-1-\overline{x} W\left(\frac{\exp\left(\frac{-1}{x}\right)}{-\overline{x}}\right) \\
& \frac{\hat{\theta}}{1-\hat{\theta}}=-1-\overline{x} W\left(\frac{\exp\left(\frac{-1}{x}\right)}{-\overline{x}}\right) \\
& \hat{\theta}=\frac{\left(-1-\overline{x} W\left(\frac{\exp\left(\frac{-1}{x}\right)}{-\overline{x}}\right)\right)}{\left(-\overline{x} W\left(\frac{\exp\left(\frac{-1}{x}\right)}{-\overline{x}}\right)\right)} \\
&\hat{\theta} = \left({\overline{x} W\left(\frac{\exp\left(\frac{-1}{x}\right)}{-\overline{x}}\right)}\right)^{-1}  + 1
&&
\end{flalign}
$$
###### Оценка методом моментов
$a_{1} = \frac{-{\theta}}{(1-{\theta})\ln(1-{\theta})}$
$\hat{a}_{1} = \overline{x}$
$\overline{x} = \frac{-\hat{\theta}}{(1-\hat{\theta})\ln(1-\hat{\theta})}$
Аналогично с м.м.п. выражается $\hat{\theta}$ и получается такая же оценка.

###### Оптимальная оценка
Логарифмическая модель - регулярна, так что найдем эффективную оценку.
$V =\frac{\partial \ln L}{\partial {\theta}} = \sum_{i=1}^{n} \left[\frac{-{\theta}^{x_{i}-1} x_{i}}{-{\theta}^{x_{i}}} + \frac{1}{(1-{\theta})\ln(1-{\theta})}\right]$
$\frac{1}{\theta}\sum_{i=1}^{n} { x_{i}} + \frac{n}{(1-{\theta})\ln(1-{\theta})} = V$
$\frac{1}{n}\sum_{i=1}^{n} { x_{i}} + \frac{\theta}{(1-{\theta})\ln(1-{\theta})} = \theta V$
Получили, что $\overline{x}$ является оптимальной оценкой для $\frac{-\theta}{(1-\theta)\ln(1-\theta)}$. Также, как и в предыдущих пунктах можно выразить $\theta$ с помощью функции Ламберта.



###### Значения оценок для сгенерированных выборок
Для каждой сгенерированной выборки необходимо привести значения полученных оценок. (решение в matstat.ipynb)

